# FGSM Adversarial Attack (Fast Gradient Sign Method)

This project demonstrates the **Fast Gradient Sign Method (FGSM)** for generating adversarial examples against an image classification model.  
We use **Inception v3** pretrained on ImageNet to show how small, imperceptible perturbations can cause a model to misclassify an image.

---

## ğŸ“‚ Project Structure
```
.
â”œâ”€â”€ fgsm_attack.py       # Main FGSM attack script
â”œâ”€â”€ dog.jpg              # Sample input image
â”œâ”€â”€ adv_image.png        # Generated adversarial image (output)
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md            # Project documentation
```

---

## ğŸš€ How to Run

### 1. Clone the repository
```bash
git clone https://github.com/your-username/fgsm-attack-demo.git
cd fgsm-attack-demo
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the script
```bash
python fgsm_attack.py
```

---

## âš™ï¸ Parameters
You can edit the following in `fgsm_attack.py`:
- `IMG_FILENAME` â†’ Path to the original image.
- `EPSILON` â†’ Perturbation size (e.g., 0.01, 0.02, 0.05).
- `ADV_OUT_FILENAME` â†’ Output filename for adversarial image.

---

## ğŸ“Š Example Output

| Original Image | Adversarial Image |
| -------------- | ----------------- |
| ![Original](dog.jpg) | ![Adversarial](adv_image.png) |

---

## ğŸ“š References
- **FGSM Paper**: [Explaining and Harnessing Adversarial Examples (Goodfellow et al., 2014)](https://arxiv.org/abs/1412.6572)
- **PyTorch Models**: [Torchvision Model Zoo](https://pytorch.org/vision/stable/models.html)

---

## ğŸ“œ License
This project is released under the MIT License.
